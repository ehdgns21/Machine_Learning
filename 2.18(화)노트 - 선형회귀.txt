2.18(화)노트 - 선형회귀


<머신러닝>
 - 머신러닝은 학문이고 이론적. 코딩은 별로 안씀
 - 실제 : 크롤링 > DB적재 > DB전처리 > DB열어서 모델 선택 >
 - 모델링은 그냥 입력하고 몇일 기다림
 - tensorflow만 GPU 사용가능하다
 - 젤 마지막과정 CNN은 코드는 실제로 10줄이 안됨
 - tensorflow는 머신러닝에도 쓸수 있고 Tree만 빼고 다 됨.
 - architecture가 되려면 이론적인걸 쌓아야함
 
 
* 선형회귀 특징
 - 데이터가 많을수록 좋다
 - 저차원모델에서는 불리하다
 - 딥러닝의 기초가 됨 : 나중에 모델, 공식 그대로 사용됨
 - 딥러닝은 선형회귀의 집합이라 할수 있다
 - 선형회귀는 이후에 관련된 모델들이 많다 vs SVM, RF 는 단일
 

* 선형회귀 단점 : 시각화가 힘들다
 - 1차원 : 점 / 2차원 : 선 / 3차원 : 면 을 그린다
 - 사이즈를 체크하는 수준
 - 간접적으로 scoring을 보는 정도


[선형회귀 모델의 예측함수] Linear Regression

y = w[0]∗x[0] + b  (특성1개)

y = w[0]∗x[0] + w[1]∗x[1] + ...+ w[p]∗x[p] + b  (특성여러개)


- 모델 파라미터 : w 와 b - 학습되어 변경됨
- 하이퍼 파라미터(Hyper Parameter) : 직접 설정해 줘야 하는 파라미터를 

* w(weight) = 가중치, 힘, 기울기 : 입력데이터에 힘을 조절하는 역할
 - 머신러닝 모델이 직접구함
 - w가 높아지면 복잡도↑ > 민감도↑ > 과대적합 가능성↑
 
* p는 특성의 계수 : csv파일의 컬럼의 개수 / 이미지파일의 픽셀 개수
 
 
y = w[0]∗x[0] + b


* import

#필요 라이브러리 임포트 복붙
from IPython.display import display
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import mglearn

 
 
[최소 제곱법(OLS, Ordinary Least Squares)]

* 선형모델의 목적은 MSE(평균 제곱 오차)의 값을 최소로 줄이는 W와 B값을 찾는게 목적
 
 
* 평균 제곱 오차(MSE - Mean Squared Error) - 랏쏘방식에서 씀
 
 MSE = 1/n ∑(i=1→n) (yi−yi)^2
                 (타겟값 - 예측값)

y는 타깃갑(=정답=입력된값)

cf. MAE(Mean Absoltute Error)도 있다 : 제곱대신 절대값
 
 
 MSE는 지수함수로 기울기가 변하므로
 > 미분을 하게되면 오차가 최소가 되는 방향으로 유도할수 있다
 > GD(Gradient Decent) 경사하강법


* RMSE(Root Mean Squared Error) - 릿지방식에서 씀


LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)

 - 언더바(_)가 붙은건 스스로 학습한 


lr의 계수(weight 또는 cofficient) : [0.39390555]
 - 배열형태로 나옴
 

* Ridge와 Lasso 특징
 - 데이터가 많을때 선형회귀를 쓰는게 나을수도 : 실제 플젝에서 5만~10만건 이상 모았을때
 
1 Ridge방식(더 많이 씀)
 - MSE
 - L2 NORM
 - 모든 특성의 가중치를 0에 수렴하도록 만듬(0은 안만듬) : 입력된 특징을 다 사용
 - y = w[0]∗x[0] + w[1]∗x[1] + b 에서 w[0]을 0입력은 안해서 다씀
 
2 Lasso방식(거의 사용x)
 - MAE
 - L1 NORM
 - 특성 자동 선택 : w(계수)를 0으로 만들수도
 - 필요없는 특성이 많을때만 사용 고려

1 2 공통점 : alpha(조절parameter)로 가중치 조절가능

 - 가중치를 조절하는 parameter : alpha(penalty)값 

cf. L1,L2 norm 섞어쓰는것 : 엘라스틴 norm


* alpha의 특징
 - alpha : w(가중치) 억제 
 - alpha ↑ : w ↓

 
 
* Ridge : 경사하강법 (미분을 통해 오차가 최소화 되는 기울기를 찾음)
* Lasso : 좌표하강법 (특성 하나의 오차에 대해 좌표축을 따라 오차가 최소화 되는 곳을 찾음)

- 즉 좌표하강법은 학습 과정이 여러번 진행 되어야 하기 때문에 max_iter 값을 지정하여
- 최소화된 오차를 계속 계산할 수 있도록 해야 한다.
- alpha를 줄이게 되면 가장 낮은 오차를 찾아가는 반복횟수가 늘어난다.

lasso001 = Lasso(alpha=0.01, max_iter=1000000).fit(X_train, y_train)











 
 
 








